{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "KC48DUucJ9LS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import cv2\n",
        "import random\n",
        "import os\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "YXIevJgM0lqU"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class FaceEncodingModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceEncodingModule, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc = nn.Linear(256 * 7 * 7, 256)  # Adjusted input size for the linear layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Face detection and blackening boxes\n",
        "        x = self.detect_faces_and_blacken_boxes(x)\n",
        "        # Preprocessing\n",
        "        x = self.preprocess_image(x)\n",
        "        # Convolutional layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool5(x)\n",
        "        # Flatten the feature map\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Apply the linear layer to reduce the dimensionality to 256\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def detect_faces_and_blacken_boxes(self, image):\n",
        "        image = np.uint8(image)\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "        # image = cv2.imread(image_path)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "        new_image = cv2.copyMakeBorder(image, 0, 0, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "        cropped_image = []\n",
        "        for (x, y, w, h) in faces:\n",
        "            cropped_image = image[y:y+h, x:x+w]\n",
        "        return cropped_image\n",
        "\n",
        "    def preprocess_image(self, cropped_image):\n",
        "        # Convert image to PIL Image\n",
        "        pil_image = Image.fromarray(cropped_image)\n",
        "        # Apply transformations\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Assuming your model expects input size of 224x224\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        image = transform(pil_image)\n",
        "        # If the image has only one channel, expand it to three channels\n",
        "        if image.shape[0] == 1:\n",
        "            image = torch.cat([image] * 3)\n",
        "        # Add batch dimension\n",
        "        image = image.unsqueeze(0)\n",
        "        return image\n",
        "\n",
        "# # Example usage:\n",
        "# # Load image\n",
        "# image = cv2.imread('/content/COCO_train2014_000000004180.jpg')\n",
        "# # Create an instance of the FaceEncodingModule\n",
        "# face_encoding_model = FaceEncodingModule()\n",
        "# # Forward pass\n",
        "# output = face_encoding_model(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "nWmwRPi5CMBS"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class ContextEncodingStream(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super(ContextEncodingStream, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        return x\n",
        "\n",
        "class AttentionInferenceModule(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super(AttentionInferenceModule, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 1, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        return x\n",
        "\n",
        "class ContextAttentionModule(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super(ContextAttentionModule, self).__init__()\n",
        "        self.context_stream = ContextEncodingStream(input_channels)\n",
        "        self.attention_inference = AttentionInferenceModule(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Face detection and blackening boxes\n",
        "        x = self.detect_faces_and_blacken_boxes(x)\n",
        "        # Preprocessing\n",
        "        x = self.preprocess_image(x)\n",
        "        context_features = self.context_stream(x)\n",
        "        attention = self.attention_inference(context_features)\n",
        "        attention = attention.squeeze(1)  # Remove singleton dimension\n",
        "        attention = F.softmax(attention, dim=1)  # Apply softmax along the channel dimension\n",
        "        context_attention = attention * context_features\n",
        "        context_attention = torch.mean(context_attention, dim=(2, 3))\n",
        "        return context_attention\n",
        "\n",
        "    def detect_faces_and_blacken_boxes(self, image):\n",
        "      image = np.uint8(image)\n",
        "    # Continue with face detection and blackening boxes\n",
        "      face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "      faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "      new_image = cv2.copyMakeBorder(image, 0, 0, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "      for (x, y, w, h) in faces:\n",
        "          new_image[y:y+h, x:x+w] = [0,0,0]\n",
        "\n",
        "      return new_image\n",
        "\n",
        "\n",
        "    def preprocess_image(self, new_image):\n",
        "        # Convert image to PIL Image\n",
        "        pil_image = Image.fromarray(new_image)\n",
        "        # Apply transformations\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Assuming your model expects input size of 224x224\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        image = transform(pil_image)\n",
        "        # If the image has only one channel, expand it to three channels\n",
        "        if image.shape[0] == 1:\n",
        "            image = torch.cat([image] * 3)\n",
        "        # Add batch dimension\n",
        "        image = image.unsqueeze(0)\n",
        "        return image\n",
        "\n",
        "# Example usage:\n",
        "# # Load image\n",
        "# image = cv2.imread('/content/COCO_train2014_000000004180.jpg')\n",
        "# # Create an instance of the ContextAttentionModule\n",
        "# context_attention_model = ContextAttentionModule(input_channels=3)\n",
        "# # Forward pass\n",
        "# output = context_attention_model(image)\n",
        "\n",
        "# output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "6a5-L_zVEyBn"
      },
      "outputs": [],
      "source": [
        "class AdaptiveFusionNetwork2D(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(AdaptiveFusionNetwork2D, self).__init__()\n",
        "        # Define convolutional layers for fusion attention\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=128, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1)\n",
        "        # Define convolutional layers for final classification\n",
        "        self.conv5 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1)  # Adjusted input_channels to 512\n",
        "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=num_classes, kernel_size=1)\n",
        "        # Define ReLU and dropout layers\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        # Define softmax layer\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        # Initialize FaceEncodingModule and ContextAttentionModule\n",
        "        self.face_encoder = FaceEncodingModule()\n",
        "        self.context_attention = ContextAttentionModule(input_channels=3)  # Assuming input_channels is 3 for RGB images\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Extract face features\n",
        "        face_features = self.face_encoder(image)\n",
        "        # Extract context attention\n",
        "        context_attention = self.context_attention(image)\n",
        "\n",
        "        # Expand dimensions of face features and context attention to match each other\n",
        "        face_features = face_features.unsqueeze(2).unsqueeze(3)\n",
        "        context_attention = context_attention.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        # Concatenate features from face and context encoding modules\n",
        "        x_concat = torch.cat((face_features, context_attention), dim=1)  # Concatenate along the channel dimension\n",
        "\n",
        "        # Apply convolutional layers for fusion attention\n",
        "        lambda_face = self.softmax(self.conv4(self.relu(self.conv3(self.relu(self.conv2(self.relu(self.conv1(x_concat))))))))\n",
        "        lambda_context = self.softmax(self.conv4(self.relu(self.conv3(self.relu(self.conv2(self.relu(self.conv1(x_concat))))))))\n",
        "\n",
        "        # Concatenate features with attention weights\n",
        "        x_final = torch.cat((face_features * lambda_face, context_attention * lambda_context), dim=1)\n",
        "\n",
        "        # Ensure x_final has correct number of channels for conv5\n",
        "        x_final = self.relu(self.conv5(x_final))\n",
        "\n",
        "        x_final = self.dropout(x_final)\n",
        "        x_final = self.conv6(x_final)\n",
        "        x_final = x_final.view(x_final.size(0), -1)  # Flatten the feature map\n",
        "        return x_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIZiluV5pBvq"
      },
      "source": [
        "Defining model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "rdiFiDkIkoiN"
      },
      "outputs": [],
      "source": [
        "fusion_network = AdaptiveFusionNetwork2D(input_channels=512, num_classes=26)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cq0fe1uovC-"
      },
      "source": [
        "Upload Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "kAfng2wiot2z"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "XSVRpzcr7CCJ"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Specify the path to your zip file\n",
        "# zip_file_path = \"/content/Dataset.zip\"  # Update this with your file path\n",
        "\n",
        "# # Specify the directory where you want to extract the contents\n",
        "# extracted_dir_path = \"Dataset\"  # Update this with your desired directory path\n",
        "\n",
        "# # Check if the directory exists, if not create it\n",
        "# if not os.path.exists(extracted_dir_path):\n",
        "#     os.makedirs(extracted_dir_path)\n",
        "\n",
        "# # Unzip the file\n",
        "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "# print(\"Extraction complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcPyXogNozd0"
      },
      "source": [
        "Random images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "RNpl96IZos2Z"
      },
      "outputs": [],
      "source": [
        "# images = glob(\"/content/Dataset/Dataset/train/**/**\")\n",
        "# for i in range(9):\n",
        "#     image = random.choice(images)\n",
        "#     plt.figure(figsize=(12,12))\n",
        "#     plt.subplot(331+i)\n",
        "#     plt.imshow(cv2.imread(image));plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "WXY_nr_HJSoc"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "!pip install -q torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ifYhoyQ-JVJn",
        "outputId": "e9413c46-913d-4ace-e709-4c0318b36a60"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bp_7ut8JV7p",
        "outputId": "0140f45a-0dff-4f27-9fb2-6a4701e1b3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 6892\n",
            "    Root location: /content/Dataset/Dataset/train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           )\n",
            "Test data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 6892\n",
            "    Root location: /content/Dataset/Dataset/train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms import Resize\n",
        "\n",
        "# Define transformations for resizing\n",
        "resize_transform = Resize((224, 224))  # Specify desired height and width\n",
        "\n",
        "# Create PyTorch datasets from the ImageDataGenerator outputs with resizing\n",
        "train_data = ImageFolder(\"/content/Dataset/Dataset/train\", transform=transforms.Compose([resize_transform, ToTensor()]))\n",
        "test_data = ImageFolder(\"/content/Dataset/Dataset/train\", transform=transforms.Compose([resize_transform, ToTensor()]))\n",
        "\n",
        "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd8RAz9WJV21",
        "outputId": "61d6eab2-0a94-46e7-fa3e-c93a3d4d4f58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image tensor:\n",
            "tensor([[[0.3882, 0.3882, 0.4039,  ..., 0.0902, 0.0980, 0.1529],\n",
            "         [0.3882, 0.3922, 0.4039,  ..., 0.1137, 0.1137, 0.0863],\n",
            "         [0.4000, 0.3922, 0.4039,  ..., 0.2471, 0.1255, 0.0706],\n",
            "         ...,\n",
            "         [0.7529, 0.7490, 0.7490,  ..., 0.6863, 0.7569, 0.8588],\n",
            "         [0.7490, 0.7569, 0.7529,  ..., 0.6745, 0.7216, 0.7882],\n",
            "         [0.7412, 0.7569, 0.7451,  ..., 0.6706, 0.6902, 0.7804]],\n",
            "\n",
            "        [[0.6078, 0.6078, 0.5961,  ..., 0.1059, 0.1294, 0.1961],\n",
            "         [0.6078, 0.6078, 0.6000,  ..., 0.1294, 0.1412, 0.1255],\n",
            "         [0.6118, 0.6118, 0.6118,  ..., 0.2588, 0.1451, 0.0941],\n",
            "         ...,\n",
            "         [0.7020, 0.6980, 0.6980,  ..., 0.6392, 0.6549, 0.6784],\n",
            "         [0.6980, 0.7059, 0.7020,  ..., 0.6431, 0.6510, 0.6353],\n",
            "         [0.6902, 0.7059, 0.6941,  ..., 0.6549, 0.6392, 0.6549]],\n",
            "\n",
            "        [[0.8039, 0.8039, 0.7529,  ..., 0.0588, 0.0824, 0.1451],\n",
            "         [0.8000, 0.8039, 0.7686,  ..., 0.0431, 0.0706, 0.0667],\n",
            "         [0.8000, 0.8078, 0.8000,  ..., 0.1294, 0.0549, 0.0314],\n",
            "         ...,\n",
            "         [0.6275, 0.6235, 0.6235,  ..., 0.5333, 0.4510, 0.3765],\n",
            "         [0.6235, 0.6314, 0.6275,  ..., 0.5529, 0.4706, 0.3647],\n",
            "         [0.6157, 0.6314, 0.6196,  ..., 0.5725, 0.4824, 0.4118]]])\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Image datatype: torch.float32\n",
            "Image label: 0\n",
            "Label datatype: <class 'int'>\n"
          ]
        }
      ],
      "source": [
        "img, label = train_data[0][0], train_data[0][1]\n",
        "print(f\"Image tensor:\\n{img}\")\n",
        "print(f\"Image shape: {img.shape}\")\n",
        "print(f\"Image datatype: {img.dtype}\")\n",
        "print(f\"Image label: {label}\")\n",
        "print(f\"Label datatype: {type(label)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iydTDmcgJVuV",
        "outputId": "0b19cc12-52c4-4919-b619-2e1fb809c6ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Affection',\n",
              " 'Anger',\n",
              " 'Annoyance',\n",
              " 'Anticipation',\n",
              " 'Aversion',\n",
              " 'Confidence',\n",
              " 'Confusion',\n",
              " 'Disapproval',\n",
              " 'Disconnection',\n",
              " 'Disquietment',\n",
              " 'Embarrassment',\n",
              " 'Engagement',\n",
              " 'Esteem',\n",
              " 'Excitement',\n",
              " 'Fatigue',\n",
              " 'Fear',\n",
              " 'Happiness',\n",
              " 'Pain',\n",
              " 'Peace',\n",
              " 'Pleasure',\n",
              " 'Sadness',\n",
              " 'Sensitivity',\n",
              " 'Suffering',\n",
              " 'Surprise',\n",
              " 'Sympathy',\n",
              " 'Yearning']"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8TnoPj1J-Au",
        "outputId": "31216c78-8b41-4591-91e1-7aca56d9c8c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7ce6cfb7be80>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7ce6cfb7b2b0>)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "num_workers = os.cpu_count()\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                             num_workers=num_workers,\n",
        "                             shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                            num_workers=num_workers,\n",
        "                            shuffle=False)\n",
        "train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "\n",
        "fusion_network = AdaptiveFusionNetwork2D(input_channels=512, num_classes=26)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(fusion_network.parameters() , lr=0.001)\n",
        "\n",
        "fusion_network.train()\n",
        "\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):\n",
        "    # Print epoch number\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "\n",
        "    # Iterate through each batch in the train_dataloader\n",
        "    for images, labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        images = images.numpy()\n",
        "        images = np.squeeze(images)\n",
        "        images = np.transpose(images, (1, 2, 0))\n",
        "\n",
        "        outputs = fusion_network(images)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    fusion_network.eval()\n",
        "\n",
        "    total_accuracy = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    # Iterate through each batch in the test_dataloader\n",
        "    for images, labels in test_dataloader:\n",
        "        # Forward pass: Pass the images through the fusion_network\n",
        "        outputs = fusion_network(images)\n",
        "\n",
        "        # Compute accuracy for the current batch\n",
        "        accuracy = calculate_accuracy(outputs, labels)\n",
        "\n",
        "        # Accumulate accuracy and update total batches\n",
        "        total_accuracy += accuracy\n",
        "        total_batches += 1\n",
        "\n",
        "    # Compute average accuracy over all batches in the test dataset\n",
        "    average_accuracy = total_accuracy / total_batches\n",
        "\n",
        "    # Print or store the evaluation metrics\n",
        "    print(f'Test accuracy: {average_accuracy}')\n",
        "\n",
        "    # Set the model back to training mode for the next epoch\n",
        "    fusion_network.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- This the extra work/handling error and other part  -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca1ISwdglQI4"
      },
      "outputs": [],
      "source": [
        "# fusion_network = AdaptiveFusionNetwork2D(input_channels=512, num_classes=26)\n",
        "# model = fusion_network(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vraoTvNJ9vZ"
      },
      "outputs": [],
      "source": [
        "# summary(model=model,\n",
        "#         input_size=(64, 3, 224, 224),\n",
        "#        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#        col_width=20,\n",
        "#        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-vUNFAYJ9qa"
      },
      "outputs": [],
      "source": [
        "# loss_fn = nn.BCEWithLogitsLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHWW5WzmJ9cj"
      },
      "outputs": [],
      "source": [
        "# !pip install torcheval\n",
        "# import torcheval\n",
        "# from torcheval.metrics import BinaryAccuracy\n",
        "# def accuracy_fn(y_true: torch.Tensor, y_pred: torch.Tensor):\n",
        "#     metric = BinaryAccuracy(threshold=0.5)\n",
        "#     metric.update(y_pred, y_true)\n",
        "#     return metric.compute().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs83fILiNHFA"
      },
      "outputs": [],
      "source": [
        "# img, label = next(iter(train_dataloader))\n",
        "# pred = model(img.to(device))\n",
        "# print(pred.squeeze())\n",
        "# print(torch.sigmoid(pred.squeeze()))\n",
        "# pred.squeeze().shape, label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP_i7P9XNG_p"
      },
      "outputs": [],
      "source": [
        "# def train_step(model: torch.nn.Module,\n",
        "#               dataloader: torch.utils.data.DataLoader,\n",
        "#               loss_fn: torch.nn.Module,\n",
        "#               optimizer: torch.optim.Optimizer,\n",
        "#               accuracy_fn):\n",
        "\n",
        "#     model.train()\n",
        "\n",
        "#     train_loss, train_acc = 0, 0\n",
        "\n",
        "#     for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "\n",
        "#         y_pred = model(X).squeeze()\n",
        "\n",
        "#         loss = loss_fn(y_pred, y.float())\n",
        "#         train_loss += loss.item()\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         train_acc += accuracy_fn(y_true=y, y_pred=torch.sigmoid(y_pred))\n",
        "\n",
        "#     train_loss /= len(dataloader)\n",
        "#     train_acc /= len(dataloader)\n",
        "\n",
        "#     return train_loss, train_acc\n",
        "\n",
        "# def test_step(model: torch.nn.Module,\n",
        "#               dataloader: torch.utils.data.DataLoader,\n",
        "#               loss_fn: torch.nn.Module,\n",
        "#               accuracy_fn):\n",
        "\n",
        "#     model.eval()\n",
        "\n",
        "#     test_loss, test_acc = 0, 0\n",
        "\n",
        "#     with torch.inference_mode():\n",
        "#         for batch, (X, y) in enumerate(dataloader):\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "\n",
        "#             test_pred_logits = model(X).squeeze()\n",
        "\n",
        "#             loss = loss_fn(test_pred_logits, y.float())\n",
        "#             test_loss += loss.item()\n",
        "\n",
        "#             test_acc += accuracy_fn(y_true=y, y_pred=torch.sigmoid(test_pred_logits))\n",
        "\n",
        "#     test_loss /= len(dataloader)\n",
        "#     test_acc /= len(dataloader)\n",
        "\n",
        "#     return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v2GK9CWNG9B"
      },
      "outputs": [],
      "source": [
        "# from tqdm.auto import tqdm\n",
        "\n",
        "# def train(model: torch.nn.Module,\n",
        "#          loss_fn: torch.nn.Module,\n",
        "#          optimizer: torch.optim.Optimizer,\n",
        "#          train_dataloader: torch.utils.data.DataLoader,\n",
        "#          test_dataloader: torch.utils.data.DataLoader,\n",
        "#          accuracy_fn,\n",
        "#          epochs: int = 5):\n",
        "\n",
        "#     results = {\n",
        "#         \"train_loss\": [],\n",
        "#         \"train_acc\": [],\n",
        "#         \"test_loss\": [],\n",
        "#         \"test_acc\": []\n",
        "#     }\n",
        "\n",
        "#     for epoch in tqdm(range(epochs)):\n",
        "\n",
        "#         train_loss, train_acc = train_step(model=model,\n",
        "#                                           dataloader=train_dataloader,\n",
        "#                                           loss_fn=loss_fn,\n",
        "#                                           optimizer=optimizer,\n",
        "#                                           accuracy_fn=accuracy_fn)\n",
        "#         test_loss, test_acc = test_step(model=model,\n",
        "#                                         dataloader=test_dataloader,\n",
        "#                                         loss_fn=loss_fn,\n",
        "#                                         accuracy_fn=accuracy_fn)\n",
        "\n",
        "#         print(\n",
        "#             f\"Epoch: {epoch+1} | \"\n",
        "#             f\"train_loss: {train_loss:.4f} | \"\n",
        "#             f\"train_acc: {train_acc:.4f} | \"\n",
        "#             f\"test_loss: {test_loss:.4f} | \"\n",
        "#             f\"test_acc: {test_acc:.4f}\"\n",
        "#         )\n",
        "\n",
        "#         results[\"train_loss\"].append(train_loss)\n",
        "#         results[\"train_acc\"].append(train_acc)\n",
        "#         results[\"test_loss\"].append(test_loss)\n",
        "#         results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "#     return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hETxnR3JNGzN"
      },
      "outputs": [],
      "source": [
        "# torch.manual_seed(42)\n",
        "# torch.cuda.manual_seed(42)\n",
        "\n",
        "# from timeit import default_timer as timer\n",
        "\n",
        "# start_time = timer()\n",
        "\n",
        "# results = train(model=model,\n",
        "#                loss_fn=loss_fn,\n",
        "#                optimizer=optimizer,\n",
        "#                accuracy_fn=accuracy_fn,\n",
        "#                train_dataloader=train_dataloader,\n",
        "#                test_dataloader=test_dataloader,\n",
        "#                epochs=10)\n",
        "\n",
        "# end_time = timer()\n",
        "\n",
        "# print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
